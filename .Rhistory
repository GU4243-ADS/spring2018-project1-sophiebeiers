cp1
# ols 2
ols2 <- lm(y == "approved" ~ Employment.Length + Debt.To.Income.Ratio * Amount.Requested + LowRatio, data = training1)
load("dataset4.RData")
str(dataset4)
# dataset manipulation
summary(dataset4$Debt.To.Income.Ratio) # see one huge outlier
dataset4$Zip.Code <- as.factor(dataset4$Zip.Code)
dataset4$State <- as.factor(dataset4$State)
dataset4$Unemployed <- ifelse(dataset4$Employment.Length == -1, 1, 0) # create new variable
dataset4$Employed <- ifelse(dataset4$Employment.Length != -1, 1, 0) # new variable
dataset4$StableEmp <- ifelse(dataset4$Employment.Length > 3, 1, 0) # new stable employment variable
dataset4$LowRatio <- ifelse(dataset4$Debt.To.Income.Ratio < 20, 1, 0) # new variable for low debt
dataset4$y <- factor(dataset4$y, labels = c("no", "approved")) # make y a factor
# create training/testing
in_train1 <- createDataPartition(y = dataset4$y, p = 3 / 4, list = FALSE) # decide what is in training
training1 <- dataset4[ in_train1, ] # create training dataset
training1[training1[, "Debt.To.Income.Ratio"] > 750000,] <- NA # rid of huge outlier
testing1  <- dataset4[-in_train1, ] # create testing dataset
# creating two datasets for approve and not approve, for curiosity
approvedf <- dataset4[dataset4$y == "approved",]
rejectdf <- dataset4[dataset4$y == "no",]
# ols 1
ols1 <- lm(y == "approved" ~ Employment.Length + Debt.To.Income.Ratio + Amount.Requested + factor(State) + factor(Employed), data = training1)
y_hat_ols1 <- predict(ols1, newdata = testing1)
summary(y_hat_ols1)
class_ols1 <- as.integer(y_hat_ols1 > 0.5) # classifying
cm1 <- table(testing1$y, class_ols1) # confusion matrix
cm1
cp1 <- sum(diag(cm1)) # correct predictions
print("Number of correct predictions for ols1 model:")
cp1
# ols 2
ols2 <- lm(y == "approved" ~ Employment.Length + Debt.To.Income.Ratio * Amount.Requested + LowRatio, data = training1)
y_hat_ols2 <- predict(ols2, newdata = testing1)
summary(y_hat_ols2)
class_ols2 <- as.integer(y_hat_ols2 > 0.5) # classifying
cm2 <- table(testing1$y, class_ols2) # confusion matrix
cm2
cp2 <- sum(diag(cm2)) # correct predictions
print("Number of correct predictions for ols2 model:")
cp2
# ols 1
ols1 <- lm(y == "approved" ~ Employment.Length + Debt.To.Income.Ratio + Amount.Requested + factor(State) + factor(Employed), data = training1)
y_hat_ols1 <- predict(ols1, newdata = testing1)
summary(y_hat_ols1)
class_ols1 <- as.integer(y_hat_ols1 > 0.5) # classifying
cm1 <- table(testing1$y, class_ols1) # confusion matrix
cm1
cp1 <- sum(diag(cm1)) # correct predictions
print("Number of correct predictions for ols1 model:")
cp1
# ols 2
ols2 <- lm(y == "approved" ~ Employment.Length + Debt.To.Income.Ratio * Amount.Requested + factor(LowRatio), data = training1)
y_hat_ols2 <- predict(ols2, newdata = testing1)
summary(y_hat_ols2)
class_ols2 <- as.integer(y_hat_ols2 > 0.5) # classifying
cm2 <- table(testing1$y, class_ols2) # confusion matrix
cm2
cp2 <- sum(diag(cm2)) # correct predictions
print("Number of correct predictions for ols2 model:")
cp2
sjt.stackfrq(char)
setwd("~/Dropbox/QMSS/Data Analysis/Labs/Final")
library(foreign)
library(sjPlot)
library(MASS)
library(graphics)
library(stargazer)
#gen_lead <- read.spss("GenLead.sav")  # Reading in Pew data
#gen_lead <- as.data.frame(gen_lead)
#vars <- c("Q4A", "Q5A", "Q5B", "Q7A", "Q7B", "Q7C", "Q7D", "Q7E", "Q7F",
#"Q7G", "Q12", "Q13", "Q17", "Q25a", "Q25b", "Q25c", "Q25d", "Q25e", "Q25f",
#"Q25g", "XPARTY7", "PPAGE", "ppagecat", "PPEDUCAT", "PPGENDER", "PPMARIT", "PPETHM")
#d <- gen_lead[,vars]
#varnames <- c("discrimination", "polleaders", "busleaders", "intelligent", "decisive",
#"compassionate", "innovative", "organized", "ambitious", "honest", "easyexec",
#"easypol", "outlook", "intelligent.imp", "decisive.imp", "compassionate.imp",
#"innovative.imp", "organized.imp", "ambitious.imp", "honest.imp","polparty", "age",
#"agecat", "education", "gender", "marriage", "race")
#colnames(d) <- varnames
dta <- read.csv("dataset.csv")
numvars <- c("discrimination", "polleaders.n", "busleaders.n", "intelligent.n", "decisive.n", "compassionate.n",                             "innovative.n", "organized.n", "ambitious.n", "honest.n", "easyexec.n", "easypol.n", "outlook.n", "age", "gender")
numeric <- dta[,numvars]
charvars <- c("polparty", "marriage", "agecat", "education")
char <- dta[,charvars]
# Descriptive Tables
table1 <- sjt.df(numeric, altr.row.col = TRUE, title = "Table 1: Descriptives of Numerical Variables", skew = FALSE)
sjt.xtab(dta$gender, char)
sjt.stackfrq(char)
na.omit(char)
sjt.stackfrq(char)
summary(polr1 <- polr(as.factor(discrimination) ~ polparty + gender.n + marriage.n + agecat.1, data = dta))
stargazer(polr1)
stargazer(polr1, type = "text")
sjt.lm(polr1)
stargazer(polr1, type = "text")
?stargazer
stargazer(polr1, type = "text", style = ajs)
stargazer(polr1, type = "text", style = "ajs")
stargazer(polr1, style = "ajs")
polr1 <- polr(as.factor(discrimination) ~ polparty + gender.n + marriage.n + agecat.1, data = dta)
stargazer(polr1, type = "latex", style = "ajs")
library(kable)
install.packages("kable")
library(knitr)
?kable
kable(polr1)
kable(char)
glm1 <- (glm(as.numeric(easymen) ~ gender.n + marriage.n + millenial + liberal + education.n + race,  data = dta, family = binomial))
dta$millenial <- ifelse(dta$agecat.1 == "Millenial", 1, 0)
dta$liberal <- ifelse(dta$polparty == "Democrat", 1, 0)
dta$easymen <-dta$easypol.n == -1 & dta$easyexec.n == -1
glm1 <- (glm(as.numeric(easymen) ~ gender.n + marriage.n + millenial + liberal + education.n + race,  data = dta, family = binomial))
stargazer(lm1, lm2, lm3, lm4, type = "latex")
lm2 <- (lm(eqldecisive ~ liberal + education.n + marriage.n + millenial + gender.n + decisive.imp, data = dta))
dta$eqlintelligent <- ifelse(dta$intelligent.n == -1, 0, 1)
dta$eqlhonesty <- ifelse(dta$honest.n == -1, 0, 1)
dta$eqldecisive <- ifelse(dta$decisive.n == -1, 0, 1)
dta$eqlorganization <- ifelse(dta$organized.n == -1, 0, 1)
vars3 <- c("eqlintelligent", "eqlhonesty", "eqldecisive", "eqlorganization")
sub1 <- dta[,vars3]
cor(sub1, use = "complete.obs")
lm2 <- (lm(eqldecisive ~ liberal + education.n + marriage.n + millenial + gender.n + decisive.imp, data = dta))
lm3 <- (lm(eqlhonesty ~ liberal + education.n + marriage.n + millenial + gender.n + honest.imp, data = dta))
lm4 <- (lm(eqlintelligent ~ liberal + education.n + marriage.n + millenial + gender + intelligent.imp, data = dta))
lm5 <- (lm(eqlorganization ~ liberal + education.n + marriage.n + millenial + gender.n + organized.imp, data = dta))
stargazer(lm2, lm3, lm4, lm5, type = "latex")
stargazer(lm2, lm3, lm4, lm5, type = "text")
polr1 <- polr(as.factor(discrimination) ~ polparty + gender.n + marriage.n + agecat, data = dta)
polr1 <- polr(as.factor(discrimination) ~ polparty + gender.n + marriage.n + agecat, data = dta)
summary(polr1)
dta$eqldecisive
table(dta$eqldecisive, dta$agecat.1)
table(dta$intelligent.n, dta$agecat.1)
prop.table(dta$intelligent.n, dta$agecat.1)
lm2 <- (lm(eqldecisive ~ liberal + education.n + marriage.n + agecat + gender.n + decisive.imp, data = dta))
summary(lm2)
(.95* .25) + (.90 * .25) + (.82 * .25) + (.70 * .25)
(.95* .25) + (.90 * .25) + (.82 * .25) + (.80 * .25)
(.95* .25) + (.90 * .25) + (.82 * .25) + (.85 * .25)
(.95* .25) + (.95 * .25) + (.82 * .25) + (.82 * .25)
exp(0.422 )
exp(0.422)
exp(0.0815)
3.977 + (0.097*15) - (0.14*15) - 1.596
3.977 + (0.097*15)
3.977 - (0.14 * 15)
library(sqldf)
```{r setup, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Dropbox/QMSS/Thesis/YouthTruth Data")
library(tidyverse)
library(sqldf)
hs <- read.csv("hs_data_for_sophie.csv")
ms <- read.csv("ms_data_for_sophie.csv")
round <- read.csv("round_table.csv")
round <- dplyr::rename(round, Round = Code)
hs <- dplyr::rename(hs, grade_level = year)
hs1 <- join(hs, round, by = "Round", type = "full")
library(plyr)
round <- dplyr::rename(round, Round = Code)
hs <- dplyr::rename(hs, grade_level = year)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Dropbox/QMSS/Thesis/YouthTruth Data")
library(tidyverse)
library(sqldf)
library(plyr)
hs <- read.csv("hs_data_for_sophie.csv")
ms <- read.csv("ms_data_for_sophie.csv")
round <- read.csv("round_table.csv")
round <- dplyr::rename(round, Round = Code)
hs <- dplyr::rename(hs, grade_level = year)
hs1 <- join(hs, round, by = "Round", type = "full")
hs1 <- filter(hs1, hs1$Year >= 2013) # filter for 2013 and beyond
full_hs <- filter(hs1, hs1$bully_yes == 1|hs1$bully_yes == 0)
?sqldf
Test <- sqldf('select Year, bully_yes, from iris group by Year')
Test <- sqldf('select Year, bully_yes, from full_hs group by Year')
Test <- sqldf('select Year, bully_yes from full_hs group by Year')
Test
Test <- sqldf('select Year, sum(bully_yes) as sum from full_hs group by Year')
Test
Test <- sqldf('select Year, sum(bully_yes) as sum, count(RespondentID) as count from full_hs group by Year')
Test <- sqldf('select Year, sum(bully_yes) as sum, count(RespondentTargetID) as count from full_hs group by Year')
Test
Test <- sqldf('select Year, sum(bully_yes) as bullied, count(RespondentTargetID) as total, sum(bully_yes)/count(RespondentTargetID) as perc from full_hs group by Year')
Test
Test <- sqldf('select Year, sum(bully_yes) as bullied, count(RespondentTargetID) as total, (sum(bully_yes)/count(RespondentTargetID)) as perc from full_hs group by Year')
Test
Test <- sqldf('select Year, sum(bully_yes) as bullied, count(RespondentTargetID) as total, bullied/total as perc from full_hs group by Year')
6466/27210
Test <- sqldf('select Year, sum(bully_yes) as bullied, count(RespondentTargetID) as total, ((sum(bully_yes))/(count(RespondentTargetID))) as perc from full_hs group by Year')
Test
By_year <- sqldf('select Year, sum(bully_yes) as bullied, count(RespondentTargetID) as total from full_hs group by Year')
By_year$freq <- By_year$bullied/By_year$total
By_year
sqldf('select * where bully_yes = 1 from full_hs group by Year')
sqldf('select * where 'bully_yes' = 1 from full_hs group by Year')
sqldf('select *, where bully_yes = 1 from full_hs group by Year')
test <- sqldf('select *, where bully_yes = 1 from full_hs group by Year')
test <- sqldf('select * where bully_yes = 1 from full_hs group by Year')
test <- sqldf('select *  from full_hs where bully_yes = 1 group by Year')
test
test <- sqldf('select count(bully_yes) as bullied, Year from full_hs where bully_yes = 1 group by Year')
test
test <- sqldf('select count(bully_yes = 1) as bullied, Year from full_hs where bully_yes = 1 group by Year')
test
test <- sqldf('select count(bully_yes = 1) as bullied, Year from full_hs group by Year')
test1 <- sqldf('select count(bully_yes = 0) as not_bullied, Year from full_hs group by Year')
test1
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gapminder)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gapminder)
ggplot(gapminder, aes(x = gdpPercap, y = lifeExp))
p <- ggplot(gapminder, aes(x = gdpPercap, y = lifeExp))
p <- ggplot(gapminder, aes(x = gdpPercap, y = lifeExp))
p <- p + geom_point() # then tell it what to map
p <- p + scale_x_log10()
p <- ggplot(gapminder, aes(x = gdpPercap, y = lifeExp))
p <- p + geom_point() # then tell it what to map
p <- p + scale_x_log10()
p
p <- p + geom_point(aes(color = continent))
p
p <- ggplot(gapminder, aes(x = gdpPercap, y = lifeExp))
p <- p + geom_point(aes(color = continent)) # then tell it what to map
p <- p + scale_x_log10()
p <- p + geom_smooth() # goes through
p
install.packages("ggthemes")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gapminder)
library(ggthemes)
p <- s
s <- p
s + theme_economist()
s
install.packages("GuardianR")
library(GuardianR)
set.api.key("c00kies!")
articles <- get_guardian(keywords = "sexual+assault",
section = "world",
from.date = "2012-11-30",
to.date = "2018-01-27",
api.key = "your-key-here")
?GuardianR
articles <- get_guardian(keywords = "sexual+assault",
section = "world",
from.date = "2012-11-30",
to.date = "2018-01-27",
api.key = "ce195ea1-11e4-4778-8d5d-84fef4a3165b")
install.packages("rtimes")
library(rtimes)
?rtimes
articles2 <- as_search(q="sexual assault", begin_date = "20170901", end_date = '20180127')
NYTIMES_AS_KEY=<d35a51f336794b8c99e50627a1391d66>
NYTIMES_AS_KEY=d35a51f336794b8c99e50627a1391d66
NYTIMES_AS_KEY="d35a51f336794b8c99e50627a1391d66"
articles2 <- as_search(q="sexual assault", begin_date = "20170901", end_date = '20180127')
Sys.setenv(NYTIMES_AS_KEY = "d35a51f336794b8c99e50627a1391d66")
articles2 <- as_search(q="sexual assault", begin_date = "20170901", end_date = '20180127')
guardarticles <- read.csv("articles.csv", stringsAsFactors = FALSE)
dim(articles)
class(articles)
colnames(articles)
head(articles)
library(dplyr)
head(articles2)
articles$body
glimpse(articles$body[1])
articles <- get_guardian(keywords = "sexual+assault",
section = "world",
from.date = "2017-11-30",
to.date = "2018-01-27",
api.key = "ce195ea1-11e4-4778-8d5d-84fef4a3165b")
articles <- get_guardian(keywords = "sexual+assault",
section = "world",
from.date = "2017-09-30",
to.date = "2018-01-27",
api.key = "ce195ea1-11e4-4778-8d5d-84fef4a3165b")
head(articles)
colnames(articles)
View(articles)
guardarticles <- select(articles, id, body, webPublicationDate)
head(guardarticles$webPublicationDate)
library(tidytext)
library(lubridate)
?ymd
guardarticles$webPublicationDate <- ymd(webPublicationDate)
guardarticles$webPublicationDate <- ymd(guardarticles$webPublicationDate)
guardarticles$webPublicationDate <- ymd_hms(guardarticles$webPublicationDate)
head(guardarticles$webPublicationDate)
guardarticles <- select(articles, id, body, webPublicationDate)
View(guardarticles)
?setp
?sep
words <- unnest_tokens(guardarticles$body, word, text)
cleanFun <- function(htmlString) {
return(gsub("<.*?>", "", htmlString))
}
articles$body <- cleanFun(articles$body)
glimpse(articles$body[1])
articles$body <- iconv(articles$body, "", "ASCII", "byte")
cleanFun <- function(htmlString) {
return(gsub("<.*?>", "", htmlString))
}
articles$body <- cleanFun(articles$body)
glimpse(articles$body[1])
body <- articles$body
body <- as.data.frame(articles$body)
words <- unnest_tokens(bigram, body, token = "word")
?unnest_tokens
words <- unnest_tokens(body, commwords, body[1, ], token = "words")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
library(topicmodels)
library(wordcloud)
library(ggridges)
source("../lib/multiplot.R")
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)
setwd("~/Documents/GitHub/spring2018-project1-sophiebeiers")
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)
spooky <- read.csv('./data/spooky.csv', as.is = TRUE)
source("./lib/multiplot.R")
spooky_wrd <- spooky %>%
unnest_tokens(word, text)
spooky_wrd <- spooky_wrd %>%
anti_join(stop_words, by = "word")
p1 <- ggplot(spooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "none")
spooky$sen_length <- str_length(spooky$text)
head(spooky$sen_length)
p1
head(spooky_wrd)
spooky_wrd <- spooky %>%
unnest_tokens(word, text) %>%
count(author, word, sort = TRUE) %>%
ungroup()
head(spooky_wrd)
spooky_wrd <- spooky_wrd %>%
anti_join(stop_words, by = "word")
head(spooky_wrd)
total_wrds <- spooky_wrd %>%
group_by(author) %>%
summarize(total = sum(n))
head(total_wrds)
spooky_wrd <- left_join(spooky_wrd, total_words)
spooky_wrd <- left_join(spooky_wrd, total_wrds)
spooky_wrd <- left_join(spooky_wrd, total_wrds, by = "author")
spooky_wrd <- spooky %>%
unnest_tokens(word, text) %>%
count(author, word, sort = TRUE) %>% # sorts by most common words
ungroup()
total_wrds <- spooky_wrd %>%
group_by(author) %>%
summarize(total = sum(n))
spooky_wrd <- left_join(spooky_wrd, total_wrds, by = "author")
head(spooky_wrd)
spooky_wrd_clean <- spooky_wrd %>%
anti_join(stop_words, by = "word") # rids of stopwords
head(spooky_wrd_clean)
MWS_words <- spooky_wrd_clean %>%
filter(author == "MWS") %>%
arrange(desc(n/total))
head(MWS_words)
MWS_words <- spooky_wrd_clean %>%
filter(author == "MWS") %>%
mutate(freq = n/total) %>%
arrange(desc(freq))
head(MWS_words)
MWS_words <- spooky_wrd_clean %>%
filter(author == "MWS") %>%
mutate(freq = n/total) %>%
arrange(desc(n)) %>%
summarise([1:10, ])
MWS_words <- spooky_wrd_clean %>%
filter(author == "MWS") %>%
mutate(freq = n/total) %>%
arrange(desc(n)) %>%
filter([1:10, ])
MWS_words <- spooky_wrd_clean %>%
filter(author == "MWS") %>%
mutate(freq = n/total) %>%
arrange(desc(n)) %>%
filter(1:10, )
MWS_words <- spooky_wrd_clean %>%
filter(author == "MWS") %>%
mutate(freq = n/total, rank = row_number()) %>%
arrange(desc(n)) %>%
filter(rank <= 10)
MWS_10words <- spooky_wrd_clean %>%
filter(author == "MWS") %>%
mutate(freq = n/total, rank = row_number()) %>%
arrange(desc(n)) %>%
filter(rank <= 10)
EAP_10words <- spooky_wrd_clean %>%
filter(author == "EAP") %>%
mutate(freq = n/total, rank = row_number()) %>%
arrange(desc(n)) %>%
filter(rank <= 10)
HPL_10words <- spooky_wrd_clean %>%
filter(author == "HPL") %>%
mutate(freq = n/total, rank = row_number()) %>%
arrange(desc(n)) %>%
filter(rank <= 10)
top10 <- join_all(list(MWS_10words, EAP_10words, HPL_10words), by = 'author', type = 'full')
top10 <- cbind.data.frame(MWS_10words, EAP_10words, HPL_10words)
top10
View(top10)
?merge
top10 <- merge(MWS_10words, EAP_10words, by = "author")
View(total_wrds)
View(top10)
top10 <- merge(MWS_10words, EAP_10words, by = author)
top10 <- merge(MWS_10words, EAP_10words)
View(top10)
top10 <- rbind(HPL_10words, EAP_10words)
top10 <- rbind(top10, MWS_10words)
v1 <- top10 %>%
ggplot(aes(x = word, y = n)) +
geom_bar(aes(color = author))
v1
v1 <- top10 %>%
ggplot(aes(x = word) +
geom_bar(aes(color = author))
v1
v1 <- top10 %>%
ggplot(aes(x = word)) +
geom_bar(aes(color = author))
v1
v1 <- top10 %>%
ggplot(aes(x = word, y = n)) +
geom_line(aes(color = author))
v1
top10
v1 <- top10 %>%
ggplot(aes(x = word, y = n)) +
geom_line(aes(color = author, group = author))
v1
v1 <- top10 %>%
ggplot(aes(x = word, y = n)) +
geom_line(aes(color = author, group = author)) +
theme_light()
v1
ggplot(top10, aes(freq, fill = author)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~author, ncol = 2, scales = "free_y")
ggplot(top10, aes(word, fill = author)) +
geom_histogram(show.legend = FALSE) +
facet_wrap(~author, ncol = 2, scales = "free_y")
ggplot(top10, aes(n, fill = author)) +
geom_histogram(show.legend = FALSE) +
facet_wrap(~author, ncol = 2, scales = "free_y")
v1 <- spooky_wrd_clean %>%
ggplot(aes(x = word, y = n)) +
geom_line(aes(color = author, group = author)) +
theme_light() +
facet_wrap(~author)
v1
v1 <- spooky_wrd_clean %>%
ggplot(aes(x = word, y = n)) +
geom_line(aes(color = author, group = author)) +
theme_light() +
facet_wrap(~author)
v1
spooky_wrd_clean
top10 %>%
ggplot(aes(rank, freq, color = author)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
spooky_wrd_clean <- spooky_wrd_clean %>%
group_by(author) %>%
mutate(rank = row_number(),
freq = n/total) # added rank and frequency values
spooky_wrd_clean %>%
ggplot(aes(rank, freq, color = author)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
spooky_wrd_clean %>%
ggplot(aes(rank, freq, color = author))
spooky_wrd_clean %>%
ggplot(aes(rank, freq, color = author)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE)
spooky_wrd_clean %>%
ggplot(aes(rank, freq, color = author)) +
geom_line() +
scale_x_log10() +
scale_y_log10()
top10 %>%
ggplot(aes(rank, freq, color = author)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
top10 %>%
ggplot(aes(rank, freq, color = author)) +
geom_line(size = 1.1) +
scale_x_log10() +
scale_y_log10()
